{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense, Convolution2D, ELU, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.layers.core import SpatialDropout2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.python.control_flow_ops = tf\n",
    "backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    model.save_weights(model_name+\".h5\", True)\n",
    "    with open(model_name+'.json', 'w') as outfile:\n",
    "        json.dump(model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_model_vgg16():\n",
    "    # vgg16\n",
    "    model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    #Create input format\n",
    "    input = Input(name = 'image_input', shape=(224,224,3))\n",
    "\n",
    "    #Use the generated model \n",
    "    output_vgg16_conv = model_vgg16_conv(input)\n",
    "\n",
    "    # freeze vgg16 conv layers\n",
    "    for layer in model_vgg16_conv.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #model_vgg16_conv.summary()\n",
    "\n",
    "    # Add the fully-connected layers \n",
    "    x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "    x = Dense(256, activation='relu', name='fc1')(x)\n",
    "    x = Dense(256, activation='relu', name='fc2')(x)\n",
    "    x = Dense(1, activation='linear', name='predicton_steering')(x)\n",
    "\n",
    "    # Create model \n",
    "    return Model(input=input, output=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_model_nvidia():\n",
    "    # based on this paper:\n",
    "    # http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(24, 5, 5, border_mode=\"valid\", subsample=(2, 2), activation=\"elu\", input_shape = (66,66,3)))\n",
    "    model.add(Convolution2D(36, 5, 5, border_mode=\"valid\", subsample=(2, 2), activation=\"elu\"))\n",
    "    model.add(Convolution2D(48, 5, 5, border_mode=\"valid\", subsample=(2, 2), activation=\"elu\"))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", subsample=(1, 1), activation=\"elu\"))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", subsample=(1, 1), activation=\"elu\"))\n",
    "    model.add(Flatten(name='flatten')) \n",
    "    model.add(Dense(100, activation=\"elu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation=\"elu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation=\"elu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Create model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model_commaai():\n",
    "    model = Sequential()\n",
    "    #model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "    #        input_shape=(ch, row, col),\n",
    "    #        output_shape=(ch, row, col)))\n",
    "    model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\", input_shape = (80,160,3)))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_6 (Convolution2D)  (None, 31, 31, 24)    1824        convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 14, 14, 36)    21636       convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 5, 5, 48)      43248       convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 3, 3, 64)      27712       convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 1, 1, 64)      36928       convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 64)            0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 100)           6500        flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 100)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 50)            5050        dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 50)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 10)            510         dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 10)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             11          dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 143,419\n",
      "Trainable params: 143,419\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model=make_model_nvidia()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#log = pd.read_csv(base_path + '/data/udacity/data/' + 'driving_log.csv')\n",
    "#log = log.drop(log[abs(log['steering']) < 0.1].index)\n",
    "#log['steering']\n",
    "#len(log)\n",
    "#log[:3]\n",
    "#log = log.drop(log[abs(log['throttle']) < 0.5].index)\n",
    "#print(len(log))\n",
    "#plt.plot(log['throttle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data_log(base_path, csv_file_name, split=0.1, col_name='center', steer_mod=0.0, flip_th=None):\n",
    "    # read log file\n",
    "    log = pd.read_csv(base_path + '/' + csv_file_name)\n",
    "    \n",
    "    # drop frames with low speed\n",
    "    #log = log.drop(log[abs(log['throttle']) < 0.2].index)\n",
    "    \n",
    "    # drop rows base don flip_th\n",
    "    if flip_th is None:\n",
    "        flip = [0]*len(log)\n",
    "    else:\n",
    "        #log = log.drop(log[abs(log['steering']) < flip_th].index)\n",
    "        flip = [1]*len(log)   \n",
    "    \n",
    "    # modify img file name column to contain the full path\n",
    "    log[col_name] = log[col_name].str.replace('/Users/amakhlin/Documents/CarND/CarND-Behavioral-Cloning-Project/data/','')\n",
    "    log[col_name] = base_path + '/' + log[col_name].str.lstrip()\n",
    "    # extract img file names\n",
    "    img_names = log[col_name].tolist()\n",
    "    # extract angles\n",
    "    angles = log['steering']   \n",
    "    angles += steer_mod\n",
    "    \n",
    "    img_names_train, img_names_test, angles_train, angles_test = train_test_split(img_names, angles, test_size=split, random_state=42)\n",
    "    \n",
    "    return ( list(zip(img_names_train, angles_train, flip)), list(zip(img_names_test, angles_test, flip)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEERING_CORRECTION_LEFT_RIGHT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Udacity dataset\n",
    "\n",
    "# center images\n",
    "train_data_list, test_data_list = preprocess_data_log(base_path + '/data/udacity/data', 'driving_log.csv')\n",
    "\n",
    "# left images\n",
    "train_data_list_l, test_data_list_l = preprocess_data_log(base_path + '/data/udacity/data', 'driving_log.csv',\n",
    "                                                          col_name='left', steer_mod=STEERING_CORRECTION_LEFT_RIGHT)\n",
    "# right images\n",
    "train_data_list_r, test_data_list_r = preprocess_data_log(base_path + '/data/udacity/data', 'driving_log.csv',\n",
    "                                                          col_name='right', steer_mod=-STEERING_CORRECTION_LEFT_RIGHT)\n",
    "\n",
    "# center images to be flipped\n",
    "#train_data_list_f, test_data_list_f = preprocess_data_log(base_path + '/data/udacity/data', 'driving_log.csv',\n",
    "#                                                      flip_th=0.1)\n",
    "\n",
    "# left images to be flipped\n",
    "#train_data_list_l_f, test_data_list_l_f = preprocess_data_log(base_path + '/data/udacity/data', 'driving_log.csv',\n",
    "#                                                         col_name='left', steer_mod=steer_mod, flip_th=0.1)\n",
    "\n",
    "# right images to be flipped\n",
    "#train_data_list_r_f, test_data_list_r_f = preprocess_data_log(base_path + '/data/udacity/data', 'driving_log.csv',\n",
    "#                                                        col_name='right', steer_mod=-steer_mod, flip_th=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_list = train_data_list +\\\n",
    "                    train_data_list_l +\\\n",
    "                    train_data_list_r# +\\\n",
    "                    #train_data_list_f +\\\n",
    "                    #train_data_list_l_f +\\\n",
    "                    #train_data_list_r_f\n",
    "\n",
    "\n",
    "test_data_list = test_data_list +\\\n",
    "                    test_data_list_l +\\\n",
    "                    test_data_list_r# +\\\n",
    "                    #test_data_list_f +\\\n",
    "                    #test_data_list_l_f +\\\n",
    "                    #test_data_list_r_f"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#log = pd.read_csv(base_path + '/data/smooth_track_x2/' + 'driving_log.csv')\n",
    "log = pd.read_csv(base_path + '/data/udacity/data/' + 'driving_log.csv')\n",
    "\n",
    "im = cv2.cvtColor(cv2.imread('data/udacity/data/'+log['center'][50]), cv2.COLOR_BGR2RGB)\n",
    "#plt.figure(figsize=(12,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im)\n",
    "#im = cv2.flip(im, 1)\n",
    "#im, _ = shift_image(im, 1)\n",
    "im = im[34:-20,:,:]\n",
    "im = cv2.resize(im, (66,66), interpolation=cv2.INTER_LINEAR)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "log = pd.read_csv('data/udacity/data/driving_log.csv')\n",
    "imc = cv2.cvtColor(cv2.imread('data/udacity/data/'+log['center'][50]), cv2.COLOR_BGR2RGB)\n",
    "iml = cv2.cvtColor(cv2.imread('data/udacity/data/'+(log['left'].str.strip())[50]), cv2.COLOR_BGR2RGB)\n",
    "imr = cv2.cvtColor(cv2.imread('data/udacity/data/'+(log['right'].str.strip())[50]), cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(iml)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(imc)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(imr)\n",
    "\n",
    "# left/right camera shift corresponds to perhaps a few degrees of turn, I started with the assumption of 2 deg (0.08)\n",
    "# eventually increased it to 3 deg (0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translate(img, tx, ty):\n",
    "    rows,cols,_ = img.shape\n",
    "\n",
    "    M = np.float32([[1,0,tx],[0,1,ty]])\n",
    "    return cv2.warpAffine(img,M,(cols,rows))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "imc_shifted = translate(imc, 25, 0)\n",
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imc)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imc_shifted)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# smooth_track_x2 dataset\n",
    "#path = base_path + '/data/smooth_track_x2'\n",
    "\n",
    "# center images\n",
    "#tr, te = preprocess_data_log(path, 'driving_log.csv')\n",
    "\n",
    "# left images\n",
    "#tr_l, te_l = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                col_name='left', steer_mod=steer_mod)\n",
    "# right images\n",
    "#tr_r, te_r = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                col_name='right', steer_mod=-steer_mod)\n",
    "\n",
    "# center images to be flipped\n",
    "#tr_f, te_f = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                    flip_th=0.1)\n",
    "\n",
    "# left images to be flipped\n",
    "#tr_l_f, te_l_f = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                        col_name='left', steer_mod=steer_mod, flip_th=0.1)\n",
    "\n",
    "# right images to be flipped\n",
    "#tr_r_f, te_r_f = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                        col_name='right', steer_mod=-steer_mod, flip_th=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#train_data_list += tr + tr_l + tr_r + tr_f + tr_l_f + tr_r_f\n",
    "\n",
    "#test_data_list += te + te_l + te_r + te_f + te_l_f + te_r_f"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# var_track_x4 dataset\n",
    "#path = base_path + '/data/var_track_x4'\n",
    "\n",
    "# center images\n",
    "#tr, te = preprocess_data_log(path, 'driving_log.csv')\n",
    "\n",
    "# left images\n",
    "#tr_l, te_l = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                col_name='left', steer_mod=steer_mod)\n",
    "# right images\n",
    "#tr_r, te_r = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                col_name='right', steer_mod=-steer_mod)\n",
    "\n",
    "# center images to be flipped\n",
    "#tr_f, te_f = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                    flip_th=0.1)\n",
    "\n",
    "# left images to be flipped\n",
    "#tr_l_f, te_l_f = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                        col_name='left', steer_mod=steer_mod, flip_th=0.1)\n",
    "\n",
    "# right images to be flipped\n",
    "#tr_r_f, te_r_f = preprocess_data_log(path, 'driving_log.csv',\n",
    "#                                        col_name='right', steer_mod=-steer_mod, flip_th=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data_list += tr + tr_f #tr + tr_l + tr_r + tr_f + tr_l_f + tr_r_f\n",
    "\n",
    "#test_data_list += te + te_f #te + te_l + te_r + te_f + te_l_f + te_r_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data_list_f[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "my_model.compile(optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_shift(im, ang, max_x_shift = 50, max_y_shift = 25, pixels_per_steering_unit = 25./STEERING_CORRECTION_LEFT_RIGHT):\n",
    "    \n",
    "    xp = int(np.random.uniform(-1,1) * max_x_shift)\n",
    "    yp = int(np.random.uniform(-1,1) * max_y_shift)\n",
    "        \n",
    "    ang = ang + xp / pixels_per_steering_unit\n",
    "    \n",
    "    return translate(im, xp, yp), ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_gen(name, data_list, im_scale, batch_size, crop_top_px=0, crop_bot_px=0):\n",
    "    # data_list is in this form: ['file_name', angle]\n",
    "    # out_img_size is a shape of the output image: (x, y, color)\n",
    "    im_h = 160\n",
    "    im_w = 320\n",
    "    #im_size = (int((im_h-crop_top_px-crop_bot_px)*im_scale), int(im_w*im_scale), 3)\n",
    "    im_size = (66, 66, 3)\n",
    "    \n",
    "    \n",
    "    # col & rows are reversed for cv2.resize\n",
    "    #resize_shape = (out_img_size[1], out_img_size[0])\n",
    "    \n",
    "    # compensate for cropping\n",
    "    #out_img_size = tuple(np.subtract(out_img_size, (crop_top_px+crop_bot_px, 0, 0)))\n",
    "    \n",
    "    # create batch_size np arrays as placeholders for imgages and angles\n",
    "    X_train = np.empty((batch_size,) +  im_size, dtype = np.float64)\n",
    "    y_train = np.empty(batch_size, dtype = np.float64)\n",
    "    \n",
    "\n",
    "    \n",
    "    while True:\n",
    "        # shuffle data\n",
    "        shuffle(data_list)\n",
    "        for offset in range(0, len(data_list), batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_subset = data_list[offset:end]\n",
    "            for i, _ in enumerate(batch_subset):\n",
    "                # load angle\n",
    "                y_train[i] = batch_subset[i][1]\n",
    "                \n",
    "                #load image\n",
    "                im = cv2.cvtColor(cv2.imread(batch_subset[i][0]), cv2.COLOR_BGR2RGB)\n",
    "                #im = mpimg.imread(batch_subset[i][0])\n",
    "                \n",
    "                if name == 'train_gen':\n",
    "                    # flip images randomly\n",
    "                    if np.random.choice([True, False]):\n",
    "                        #if batch_subset[i][2] == 1:\n",
    "                        im = cv2.flip(im, 1)\n",
    "                        y_train[i] *= -1.\n",
    "\n",
    "                    # Randomly translate the image to the left or right\n",
    "                    #im, y_train[i] = shift_image(im, y_train[i])\n",
    "                    im, y_train[i] = random_shift(im, y_train[i])\n",
    "                    \n",
    "                # crop - img[y: y + h, x: x + w]\n",
    "                #im = im[crop_top_px:(im_h-crop_bot_px),:,:]\n",
    "                im = im[34:-20,:,:]\n",
    "                    \n",
    "                # resize and norm\n",
    "                #X_train[i] = (cv2.resize(im, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_AREA).astype(np.float32))/254. - 0.5\n",
    "                \n",
    "                X_train[i] = (cv2.resize(im, (66,66), interpolation=cv2.INTER_LINEAR).astype(np.float64))/255. - 0.5\n",
    "                \n",
    "\n",
    "                \n",
    "                #print(im_arr.shape)\n",
    "                # crop - img[y: y + h, x: x + w]\n",
    "                #print(im_arr.shape)\n",
    "                # X_train[i]\n",
    "\n",
    "                #X_train[i] = np.expand_dims(im, axis=0)\n",
    "                \n",
    "                \n",
    "            batch_X, batch_y = X_train[:len(batch_subset)], y_train[:len(batch_subset)]\n",
    "            #print(name)\n",
    "            yield (batch_X, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#g=batch_gen(\"train_gen\", train_data_list, 0.5, 1, 40, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(im, a) = next(g)\n",
    "#im.shape\n",
    "#im = np.load('float_img.npy')\n",
    "#plt.imshow(cv2.cvtColor(((im[0]+1)*128.).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "#spplt.imshow(((im+1.)*128.).astype(np.uint8))\n",
    "#plt.figure(figsize=(8,4))\n",
    "#plt.imshow(((im[0]+0.5)*255.).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21696"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(im, a) = next(g)\n",
    "len(train_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2412"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.imshow(cv2.cvtColor((im[0]*255.+128.).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "#im0 = cv2.resize(cv2.imread(train_data_list[0][0]), (224, 224))\n",
    "#plt.imshow(cv2.cvtColor(im0, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = batch_gen(\"train_gen\", train_data_list, 0.5, 64, 40, 20)\n",
    "test_gen = batch_gen(\"test_gen\", test_data_list, 0.5, 64, 40, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path=\"nvidia-{epoch:02d}.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=False, save_weights_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0855Epoch 00000: saving model to nvidia-00.h5\n",
      "21696/21696 [==============================] - 41s - loss: 0.0854 - val_loss: 0.0442\n",
      "Epoch 2/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0716Epoch 00001: saving model to nvidia-01.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0716 - val_loss: 0.0525\n",
      "Epoch 3/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0639Epoch 00002: saving model to nvidia-02.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0638 - val_loss: 0.0364\n",
      "Epoch 4/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0582Epoch 00003: saving model to nvidia-03.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0582 - val_loss: 0.0316\n",
      "Epoch 5/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0535Epoch 00004: saving model to nvidia-04.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0535 - val_loss: 0.0350\n",
      "Epoch 6/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0518Epoch 00005: saving model to nvidia-05.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0518 - val_loss: 0.0337\n",
      "Epoch 7/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0497Epoch 00006: saving model to nvidia-06.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0496 - val_loss: 0.0304\n",
      "Epoch 8/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0486Epoch 00007: saving model to nvidia-07.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0486 - val_loss: 0.0356\n",
      "Epoch 9/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0481Epoch 00008: saving model to nvidia-08.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0481 - val_loss: 0.0294\n",
      "Epoch 10/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0482Epoch 00009: saving model to nvidia-09.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0482 - val_loss: 0.0283\n",
      "Epoch 11/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0473Epoch 00010: saving model to nvidia-10.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0473 - val_loss: 0.0321\n",
      "Epoch 12/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0462Epoch 00011: saving model to nvidia-11.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0462 - val_loss: 0.0330\n",
      "Epoch 13/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0455Epoch 00012: saving model to nvidia-12.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0455 - val_loss: 0.0285\n",
      "Epoch 14/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0449Epoch 00013: saving model to nvidia-13.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0449 - val_loss: 0.0288\n",
      "Epoch 15/15\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0448Epoch 00014: saving model to nvidia-14.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0448 - val_loss: 0.0291\n"
     ]
    }
   ],
   "source": [
    "history = my_model.fit_generator(\n",
    "    generator=train_gen, \n",
    "    validation_data=test_gen, \n",
    "    nb_val_samples=len(test_data_list), \n",
    "    samples_per_epoch=len(train_data_list), \n",
    "    nb_epoch=15,\n",
    "    max_q_size=2,\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_model(my_model, 'nvidia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0439Epoch 00000: saving model to nvidia-00.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0439 - val_loss: 0.0268\n",
      "Epoch 2/5\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0440Epoch 00001: saving model to nvidia-01.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0440 - val_loss: 0.0287\n",
      "Epoch 3/5\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0430Epoch 00002: saving model to nvidia-02.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0430 - val_loss: 0.0280\n",
      "Epoch 4/5\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0435Epoch 00003: saving model to nvidia-03.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0434 - val_loss: 0.0256\n",
      "Epoch 5/5\n",
      "21632/21696 [============================>.] - ETA: 0s - loss: 0.0426Epoch 00004: saving model to nvidia-04.h5\n",
      "21696/21696 [==============================] - 39s - loss: 0.0427 - val_loss: 0.0258\n"
     ]
    }
   ],
   "source": [
    "history = my_model.fit_generator(\n",
    "    generator=train_gen, \n",
    "    validation_data=test_gen, \n",
    "    nb_val_samples=len(test_data_list), \n",
    "    samples_per_epoch=len(train_data_list), \n",
    "    nb_epoch=5,\n",
    "    max_q_size=2,\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(my_model, 'nvidia')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
